// inline fn ghash(reg u128 input, reg u128 h, reg u32 i) -> reg u128 {
// 	// H is the hash key
// 	// A is the additional authenticated data
// 	// C is the ciphertext

// 	reg u128 hash_value;
// 	// Initialize the hash value to zero
// 	hash_value = #set0_128();

// 	// Process the additional authenticated data (AAD)
// 	// for each block a_block in A:
// 	// 	hash_value = multiply_in_gf(hash_value, H)
// 	// 	hash_value = xor(hash_value, a_block)
// 	hash_value = #VPXOR(hash_value, a); // xor
// 	hash_value = #VPCLMULQDQ(hash_value, h, 128); // mult GF(2^128)

// 	// Process the ciphertext
// 	// for each block c_block in C:
// 	// 	hash_value = multiply_in_gf(hash_value, H)
// 	// 	hash_value = xor(hash_value, c_block)
// 	hash_value = #VPXOR(hash_value, c); // xor
// 	hash_value = #VPCLMULQDQ(hash_value, h, 128); // mult GF(2^128)

// 	return hash_value;
// }

inline fn ghash(reg u128 input, reg u128 h, reg u32 i) -> reg u128 {
	reg u128 hash_value;
	hash_value = input;

	// VPCLMULQDQ https://www.felixcloutier.com/x86/pclmulqdq

	// reg u128 val2;
	// temp64 = 4;
	// val2 = #VPINSR_2u64(val2, temp64, 1);
	// temp64 = 5;
	// val2 = #VPINSR_2u64(val2, temp64, 0);

	// https://www.intel.com/content/dam/develop/external/us/en/documents/clmul-wp-rev-2-02-2014-04-20.pdf
	// Karatsuba multiplication (Intel spec page 13 algo 2)
	// We want to multiply two 128-bit numbers: [A1:A0] and [B1:B0]
	// [C1:C0] = A1*B1
	// [D1:D0] = A0*B0
	// [E1:E0] = (A0⊕A1)*(B0⊕B1)

	reg u128 a; a = input;
	reg u128 b; b = h;

	reg u128 c; c = #VPCLMULQDQ(a, b, 0x11);
	reg u128 d; d = #VPCLMULQDQ(a, b, 0x00);

	reg u64 a0; a0 = #VPEXTR_64(a, 0);
	reg u64 a1; a1 = #VPEXTR_64(a, 1);
	reg u64 xor_a; xor_a = a0; xor_a ^= a1;
	reg u64 b0; b0 = #VPEXTR_64(b, 0);
	reg u64 b1; b1 = #VPEXTR_64(b, 1);
	reg u64 xor_b; xor_b = b0; xor_b ^= b1;
	reg u128 xor_a_128; xor_a_128 = (128u) xor_a;
	reg u128 xor_b_128; xor_b_128 = (128u) xor_b;
	reg u128 e; e = #VPCLMULQDQ(xor_a_128, xor_b_128, 0x00);
	

	// Reconstruction of the result (256 bits)
	// [A1:A0]*[B1:B0] = [X3:X2:X1:X0] = [C1 : C0⊕C1⊕D1⊕E1 : D1⊕C0⊕D0⊕E0 : D0]
	reg u64 x3 x2 x1 x0;
	x0 = #VPEXTR_64(d, 0);
	
	reg u64 temp1;
	
	x1 = #VPEXTR_64(d, 1);
	temp1 = #VPEXTR_64(c, 0);
	x1 ^= temp1;
	temp1 = #VPEXTR_64(d, 0);
	x1 ^= temp1;
	temp1 = #VPEXTR_64(e, 0);
	x1 ^= temp1;

	x2 = #VPEXTR_64(c, 0);
	temp1 = #VPEXTR_64(c, 1);
	x2 ^= temp1;
	temp1 = #VPEXTR_64(d, 1);
	x2 ^= temp1;
	temp1 = #VPEXTR_64(e, 1);
	x2 ^= temp1;

	x3 = #VPEXTR_64(c, 1);

	// -------------------------------------------------
	// Reduction of the 256-bit output
	// Page 17 algo 4 // TODO
	// Step 1
	reg u64 a_64 b_64 c_64;
	a_64 = x3;
	a_64 >>= 63;
	b_64 = x3;
	b_64 >>= 62;
	c_64 = x3;
	c_64 >>= 57;

	// Step 2
	reg u64 d_64;
	d_64 = x2;
	d_64 ^= a_64;
	d_64 ^= b_64;
	d_64 ^= c_64;

	// Step 3
	reg u128 e_128 f_128 g_128;
	e_128 = #VPINSR_2u64(e_128, x3, 1);
	e_128 = #VPINSR_2u64(e_128, d_64, 0);
	e_128 = #VPSLLDQ(e_128, 1);

	f_128 = #VPINSR_2u64(f_128, x3, 1);
	f_128 = #VPINSR_2u64(f_128, d_64, 0);
	f_128 = #VPSLLDQ(f_128, 2);

	g_128 = #VPINSR_2u64(g_128, x3, 1);
	g_128 = #VPINSR_2u64(g_128, d_64, 0);
	g_128 = #VPSLLDQ(g_128, 7);
	
	// Step 4
	reg u64 h0_64 h1_64 temp;
	h1_64 = x3;
	temp = #VPEXTR_64(e_128, 1);
	h1_64 ^= temp;
	temp = #VPEXTR_64(f_128, 1);
	h1_64 ^= temp;
	temp = #VPEXTR_64(g_128, 1);
	h1_64 ^= temp;

	h0_64 = d_64;
	temp = #VPEXTR_64(e_128, 0);
	h0_64 ^= temp;
	temp = #VPEXTR_64(f_128, 0);
	h0_64 ^= temp;
	temp = #VPEXTR_64(g_128, 0);
	h0_64 ^= temp;

	// -------------------------------------------------
	
	// Step 5: return
	reg u128 res;
	reg u64 res0 res1;
	res0 = x0;
	res0 ^= h0_64;
	res1 = x1;
	res1 ^= h1_64;
	res = #VPINSR_2u64(res, res0, 0);
	res = #VPINSR_2u64(res, res1, 1);

	// -------------------------------------------------


	// Insert 64-bit words into 256-bit result, but hopefully we don't need to do this
	// reg u256 f0_256; f0_256 = (256u) f0;
	// reg u256 f1_256; f1_256 = (256u) f1;
	// reg u256 f2_256; f2_256 = (256u) f2;
	// reg u256 f3_256; f3_256 = (256u) f3;
	// reg u256 f; f = #set0_256();
	// f = #VINSERTI128(f, f0_256, 0);
	// f = #VINSERTI128(f, f1_256, 1);
	// f = #VINSERTI128(f, f2_256, 2);
	// f = #VINSERTI128(f, f3_256, 3);

	return res;
}

inline fn ghash_xor(reg u128 ghash_prev, reg u128 data, reg u128 h, reg u32 i) -> reg u128 {
	reg u128 hash_value;
	hash_value = ghash_prev;

	hash_value = #VPXOR(hash_value, data); // xor


	hash_value = ghash(hash_value, h, i);

	return hash_value;
}

// inline fn everything(reg u128 p, reg u128 k, reg u128 iv0, reg u128 iv1) -> reg u128 {
// 	reg u128 hash_value;
// 	hash_value = ghash_prev;

// 	hash_value = #VPXOR(hash_value, data); // xor


// 	hash_value = ghash(hash_value, h, i);

// 	return hash_value;
// }

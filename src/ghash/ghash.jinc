/// This code defines several functions related to the GHASH algorithm used in AES-GCM encryption.
/// The GHASH algorithm is used to calculate the authentication tag for the encrypted data.
/// The functions in this code perform bitwise operations, reflection, and multiplication using the VPCLMULQDQ instruction.
/// The `bit_reflect` function reflects the bits of a 128-bit value.
/// The `bit_reflect_bis` function is an optimized version of `bit_reflect` that takes additional parameters for the mask, higher mask, and lower mask.
/// The `mult` function performs Karatsuba multiplication on two 128-bit values using the VPCLMULQDQ instruction.
/// The `ghash` function calculates the GHASH value of two 128-bit values.
/// The `ghash_xor` function calculates the GHASH value of the XOR of two 128-bit values and a third 128-bit value.
/// The `ghash_bis` function is an optimized version of `ghash` that takes additional parameters for the mask, higher mask, and lower mask.
/// The `ghash_xor_bis` function is an optimized version of `ghash_xor` that takes additional parameters for the mask, higher mask, and lower mask.

u64[2] AND_MASK = {0x0f0f0f0f0f0f0f0f, 0x0f0f0f0f0f0f0f0f};
u64[2] LOWER_MASK = {0x0f070b030d050901, 0x0e060a020c040800};
u64[2] LOWER_MASK_INV = {0x0e060a020c040800, 0x0f070b030d050901};
u64[2] HIGHER_MASK = {0xf070b030d0509010, 0xe060a020c0408000};
u64[2] HIGHER_MASK_INV = {0xe060a020c0408000, 0xf070b030d0509010};

inline fn bit_reflect(reg u128 x) -> reg u128 {
	reg u128 temp1 temp2 mask;
	mask = AND_MASK[u128 0];
	
	temp2 = x;
	temp2 = #VPSRL_8u16(temp2, 4);

	temp1 = x;
	temp1 = #VPAND_128(temp1, mask);

	temp2 = #VPAND_128(temp2, mask);

	reg u128 higher_mask;
	higher_mask = HIGHER_MASK_INV[u128 0];

	temp1 = #VPSHUFB(higher_mask, temp1);

	reg u128 lower_mask;
	lower_mask = LOWER_MASK_INV[u128 0];
	temp2 = #VPSHUFB_128(lower_mask, temp2);
	
	temp1 = #VPXOR_128(temp1, temp2);

	global u128 swap_me;
    swap_me = (16u8)[ 0, 1 , 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15];
	
	temp1 = #VPSHUFB_128(temp1, swap_me);
	
	return temp1;
}
inline fn bit_reflect_bis(reg u128 x mask higher_mask lower_mask) -> reg u128 {
	reg u128 temp1 temp2;
	
	temp2 = x;
	temp2 = #VPSRL_8u16(temp2, 4);

	temp1 = x;
	temp1 = #VPAND_128(temp1, mask);

	temp2 = #VPAND_128(temp2, mask);


	temp1 = #VPSHUFB(higher_mask, temp1);

	temp2 = #VPSHUFB_128(lower_mask, temp2);
	
	temp1 = #VPXOR_128(temp1, temp2);

	global u128 swap_me;
    swap_me = (16u8)[ 0, 1 , 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15];
	
	temp1 = #VPSHUFB_128(temp1, swap_me);
	
	return temp1;
}


inline fn mult(reg u128 a, reg u128 b) -> reg u128 {
	// VPCLMULQDQ https://www.felixcloutier.com/x86/pclmulqdq

	// https://www.intel.com/content/dam/develop/external/us/en/documents/clmul-wp-rev-2-02-2014-04-20.pdf
	// Karatsuba multiplication (Intel spec page 13 algo 2)
	// We want to multiply two 128-bit numbers: [A1:A0] and [B1:B0]
	// [C1:C0] = A1*B1
	// [D1:D0] = A0*B0
	// [E1:E0] = (A0âŠ•A1)*(B0âŠ•B1)
	
	reg u128 c; c = #set0_128(); c = #VPCLMULQDQ(a, b, 0x11);
	reg u128 d; d = #set0_128(); d = #VPCLMULQDQ(a, b, 0x00);

	reg u64 a0; a0 = 0; a0 = #VPEXTR_64(a, 0);
	reg u64 a1; a1 = 0; a1 = #VPEXTR_64(a, 1);
	reg u64 xor_a; xor_a = a0; xor_a ^= a1;
	reg u64 b0; b0 = 0; b0 = #VPEXTR_64(b, 0);
	reg u64 b1; b1 = 0; b1 = #VPEXTR_64(b, 1);
	reg u64 xor_b; xor_b = b0; xor_b ^= b1;
	reg u128 xor_a_128; xor_a_128 = (128u) xor_a;
	reg u128 xor_b_128; xor_b_128 = (128u) xor_b;
	reg u128 e; e = #set0_128(); e = #VPCLMULQDQ(xor_a_128, xor_b_128, 0x00);

	// reduction

	e = #VPXOR(e, d);
	e = #VPXOR(e, c);

	reg u128 temp temp3 temp4 temp5 temp6 temp7;
	temp = #set0_128();
	reg u64 temp2;
	temp2 = #VPEXTR_64(e, 0);
	temp2 = temp2;
	temp = #VPINSR_2u64(temp, temp2, 1);
	temp2 = 0;
	temp2 = #VPEXTR_64(e, 1);
	e = #set0_128();
	e = (128u) temp2;
	d = #VPXOR(d, temp);
	c = #VPXOR(c, e);

	temp3 = #set0_128();
	temp4 = #set0_128();
	temp3 = #VPSRL_4u32(c, 31);
	temp4 = #VPSRL_4u32(c, 30);
	temp5 = #VPSRL_4u32(c, 25);

	temp3 = #VPXOR(temp3, temp4);
	temp3 = #VPXOR(temp3, temp5);

	temp6 = #set0_128();
	temp6 = #VPSHUFD_128(temp3, 147);

	temp7 = #set0_128();
	reg u64 temp8;
	temp8 = 0xffffffff;
	temp7 = #VPINSR_2u64(temp7, temp8, 0);

	temp3 = #VPAND_128(temp7, temp6);

	temp6 = #VPANDN_128(temp7, temp6);

	d = #VPXOR(d, temp6);
	c = #VPXOR(c, temp3);

	temp7 = #set0_128();
	temp7 = #VPSLL_4u32(c, 1);
	d = #VPXOR(d, temp7);

	temp7 = #set0_128();
	temp7 = #VPSLL_4u32(c, 2);
	d = #VPXOR(d, temp7);

	temp7 = #set0_128();
	temp7 = #VPSLL_4u32(c, 7);
	d = #VPXOR(d, temp7);

	reg u128 res;
	res = #set0_128();
	res = d;
	res = #VPXOR(res, c);

	return res;
}

// 
inline fn ghash(reg u128 a, reg u128 b) -> reg u128 {
	reg u128 mask higher_mask lower_mask;
	mask = AND_MASK[u128 0];
	higher_mask = HIGHER_MASK_INV[u128 0];
	lower_mask = LOWER_MASK_INV[u128 0];

	reg u128 a_reflected; a_reflected = bit_reflect_bis(a, mask, higher_mask, lower_mask);
	reg u128 b_reflected; b_reflected = bit_reflect_bis(b, mask, higher_mask, lower_mask);
	
	reg u128 res; res = mult(a_reflected, b_reflected);

	res = bit_reflect_bis(res, mask, higher_mask, lower_mask);
	return res;
}

fn ghash_xor(reg u128 ghash_prev, reg u128 data, reg u128 h) -> reg u128 {
	reg u128 res;
	res = #VPXOR(ghash_prev, data);
	res = ghash(res, h);
	return res;
}
inline fn ghash_bis(reg u128 a, reg u128 b, reg u128 mask higher_mask lower_mask) -> reg u128 {
	reg u128 a_reflected; a_reflected = bit_reflect_bis(a, mask, higher_mask, lower_mask);
	reg u128 b_reflected; b_reflected = bit_reflect_bis(b, mask, higher_mask, lower_mask);
	
	reg u128 res; res = mult(a_reflected, b_reflected);

	res = bit_reflect_bis(res, mask, higher_mask, lower_mask);
	return res;
}

fn ghash_xor_bis(reg u128 ghash_prev, reg u128 data, reg u128 h, reg u128 mask higher_mask lower_mask) -> reg u128 {
	reg u128 res;
	res = #VPXOR(ghash_prev, data);
	res = ghash_bis(res, h, mask, higher_mask, lower_mask);
	return res;
}

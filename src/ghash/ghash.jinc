// inline fn ghash(reg u128 input, reg u128 h, reg u32 i) -> reg u128 {
// 	// H is the hash key
// 	// A is the additional authenticated data
// 	// C is the ciphertext

// 	reg u128 hash_value;
// 	// Initialize the hash value to zero
// 	hash_value = #set0_128();

// 	// Process the additional authenticated data (AAD)
// 	// for each block a_block in A:
// 	// 	hash_value = multiply_in_gf(hash_value, H)
// 	// 	hash_value = xor(hash_value, a_block)
// 	hash_value = #VPXOR(hash_value, a); // xor
// 	hash_value = #VPCLMULQDQ(hash_value, h, 128); // mult GF(2^128)

// 	// Process the ciphertext
// 	// for each block c_block in C:
// 	// 	hash_value = multiply_in_gf(hash_value, H)
// 	// 	hash_value = xor(hash_value, c_block)
// 	hash_value = #VPXOR(hash_value, c); // xor
// 	hash_value = #VPCLMULQDQ(hash_value, h, 128); // mult GF(2^128)

// 	return hash_value;
// }


u64[2] AND_MASK = {0x0f0f0f0f0f0f0f0f, 0x0f0f0f0f0f0f0f0f};
u64[2] LOWER_MASK = {0x0f070b030d050901, 0x0e060a020c040800};
u64[2] HIGHER_MASK = {0xf070b030d0509010, 0xe060a020c0408000};
u64[2] BSWAP_MASK = {0x000102030405060708,0x090a0b0c0d0e0f};


inline fn reflecting_bit_128(reg u64 ptr_x) {
	reg u128 temp1 temp2 mask;
	mask = #VPINSR_2u64(mask, AND_MASK[0], 0);
	mask = #VPINSR_2u64(mask, AND_MASK[1], 1);
	
	temp2 = (u128)[ptr_x];
	temp2 = #VPSRL_8u16(temp2, 4);

	temp1 = (u128)[ptr_x];
	temp1 = #VPAND_128(temp1, mask);

	temp2 = #VPAND_128(temp2, mask);

	reg u128 higher_mask;
	higher_mask = #VPINSR_2u64(higher_mask, HIGHER_MASK[0], 1);
	higher_mask = #VPINSR_2u64(higher_mask, HIGHER_MASK[1], 0);
	temp1 = #VPSHUFB(higher_mask, temp1);

	reg u128 lower_mask;
	lower_mask = #VPINSR_2u64(lower_mask, LOWER_MASK[0], 1);
	lower_mask = #VPINSR_2u64(lower_mask, LOWER_MASK[1], 0);
	temp2 = #VPSHUFB_128(lower_mask, temp2);
	
	temp1 = #VPXOR_128(temp1, temp2);

	global u128 swap_me;
    swap_me = (16u8)[ 0, 1 , 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15];
	
	temp1 = #VPSHUFB_128(temp1, swap_me);
	(u128)[ptr_x] = temp1;
}

inline fn ghash(reg u128 input, reg u128 h, reg u32 i) -> reg u128 {
	// VPCLMULQDQ https://www.felixcloutier.com/x86/pclmulqdq

	// https://www.intel.com/content/dam/develop/external/us/en/documents/clmul-wp-rev-2-02-2014-04-20.pdf
	// Karatsuba multiplication (Intel spec page 13 algo 2)
	// We want to multiply two 128-bit numbers: [A1:A0] and [B1:B0]
	// [C1:C0] = A1*B1
	// [D1:D0] = A0*B0
	// [E1:E0] = (A0⊕A1)*(B0⊕B1)
	reg u128 a; a = input;
	reg u128 b; b = h;
	
	reg u128 c; c = #set0_128(); c = #VPCLMULQDQ(a, b, 0x11);
	reg u128 d; d = #set0_128(); d = #VPCLMULQDQ(a, b, 0x00);

	reg u64 a0; a0 = 0; a0 = #VPEXTR_64(a, 0);
	reg u64 a1; a1 = 0; a1 = #VPEXTR_64(a, 1);
	reg u64 xor_a; xor_a = a0; xor_a ^= a1;
	reg u64 b0; b0 = 0; b0 = #VPEXTR_64(b, 0);
	reg u64 b1; b1 = 0; b1 = #VPEXTR_64(b, 1);
	reg u64 xor_b; xor_b = b0; xor_b ^= b1;
	reg u128 xor_a_128; xor_a_128 = (128u) xor_a;
	reg u128 xor_b_128; xor_b_128 = (128u) xor_b;
	reg u128 e; e = #set0_128(); e = #VPCLMULQDQ(xor_a_128, xor_b_128, 0x00);


	// Reconstruction of the result (256 bits)
	// [A1:A0]*[B1:B0] = [X3:X2:X1:X0] = [C1 : C0⊕C1⊕D1⊕E1 : D1⊕C0⊕D0⊕E0 : D0]
	reg u64 x3 x2 x1 x0;
	x0 = #VPEXTR_64(d, 0);
	
	reg u64 temp1;
	
	x1 = #VPEXTR_64(d, 1);
	temp1 = #VPEXTR_64(c, 0);
	x1 ^= temp1;
	temp1 = #VPEXTR_64(d, 0);
	x1 ^= temp1;
	temp1 = #VPEXTR_64(e, 0);
	x1 ^= temp1;

	x2 = #VPEXTR_64(c, 0);
	temp1 = #VPEXTR_64(c, 1);
	x2 ^= temp1;
	temp1 = #VPEXTR_64(d, 1);
	x2 ^= temp1;
	temp1 = #VPEXTR_64(e, 1);
	x2 ^= temp1;

	x3 = #VPEXTR_64(c, 1);

	// -------------------------------------------------
	// Reduction of the 256-bit output
	// Page 17 algo 4 // TODO
	// Step 1
	reg u64 a_64 b_64 c_64;
	a_64 = x3;
	a_64 >>= 63;
	b_64 = x3;
	b_64 >>= 62;
	c_64 = x3;
	c_64 >>= 57;

	// Step 2
	reg u64 d_64;
	d_64 = x2;
	d_64 ^= a_64;
	d_64 ^= b_64;
	d_64 ^= c_64;

	// Step 3
	reg u128 e_128 f_128 g_128;
	e_128 = #set0_128();
	f_128 = #set0_128();
	g_128 = #set0_128();

	
	e_128 = #VPINSR_2u64(e_128, x3, 1);
	e_128 = #VPINSR_2u64(e_128, d_64, 0);
	e_128 = #VPSLL_2u64(e_128, 1);
	
	f_128 = #VPINSR_2u64(f_128, x3, 1);
	f_128 = #VPINSR_2u64(f_128, d_64, 0);
	f_128 = #VPSLL_2u64(f_128, 2);
	
	g_128 = #VPINSR_2u64(g_128, x3, 1);
	g_128 = #VPINSR_2u64(g_128, d_64, 0);
	g_128 = #VPSLL_2u64(g_128, 7);
	
	// Step 4
	reg u64 h0_64 h1_64 temp;
	h0_64 = 0;
	h1_64 = 0;
	temp = 0;
		
	h1_64 = x3;
	temp = #VPEXTR_64(e_128, 1);
	h1_64 ^= temp;
	temp = #VPEXTR_64(f_128, 1);
	h1_64 ^= temp;
	temp = #VPEXTR_64(g_128, 1);
	h1_64 ^= temp;

	h0_64 = d_64;
	temp = #VPEXTR_64(e_128, 0);
	h0_64 ^= temp;
	temp = #VPEXTR_64(f_128, 0);
	h0_64 ^= temp;
	temp = #VPEXTR_64(g_128, 0);
	h0_64 ^= temp;

	// -------------------------------------------------


	// Step 5: return
	reg u128 res;
	res = #set0_128();
	reg u64 res0 res1;
	res0 = x0;
	res0 ^= h0_64;
	res1 = x1;
	res1 ^= h1_64;
	res = #VPINSR_2u64(res, res0, 0);
	res = #VPINSR_2u64(res, res1, 1);

	return res;
}

inline fn ghash_xor(reg u128 ghash_prev, reg u128 data, reg u128 h, reg u32 i) -> reg u128 {
	reg u128 hash_value;
	hash_value = ghash_prev;

	hash_value = #VPXOR(hash_value, data); // xor


	hash_value = ghash(hash_value, h, i);

	return hash_value;
}

// inline fn everything(reg u128 p, reg u128 k, reg u128 iv0, reg u128 iv1) -> reg u128 {
// 	reg u128 hash_value;
// 	hash_value = ghash_prev;

// 	hash_value = #VPXOR(hash_value, data); // xor


// 	hash_value = ghash(hash_value, h, i);

// 	return hash_value;
// }
